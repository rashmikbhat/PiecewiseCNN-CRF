{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17f70828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages...\n",
      "======================================================================\n",
      "Looking in indexes: https://boartifactory.micron.com/artifactory/api/pypi/micron-pypi-rel-virtual/simple\n",
      "Requirement already satisfied: torch in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.22.1+cu118)\n",
      "Requirement already satisfied: numpy in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (11.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.65.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: tabulate in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (60.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.59.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rkekanaje\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "âœ… All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 - Install Dependencies\n",
    "\n",
    "print(\"Installing required packages...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Install packages\n",
    "%pip install torch torchvision numpy pillow matplotlib tqdm pandas tabulate seaborn \n",
    "\n",
    "print(\"\\nâœ… All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc8b3bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n",
      "PyTorch version: 2.7.1+cu118\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA RTX 2000 Ada Generation Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 - Import Libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vgg16\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f08e52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Data source import complete.\n",
      "Dataset downloaded to: C:\\Users\\rkekanaje\\.cache\\kagglehub\\datasets\\huanghanchina\\pascal-voc-2012\\versions\\1\n",
      "âœ… Image directory: C:\\Users\\rkekanaje\\.cache\\kagglehub\\datasets\\huanghanchina\\pascal-voc-2012\\versions\\1\\VOC2012\\JPEGImages\n",
      "âœ… Label directory: C:\\Users\\rkekanaje\\.cache\\kagglehub\\datasets\\huanghanchina\\pascal-voc-2012\\versions\\1\\VOC2012\\SegmentationClass\n",
      "\n",
      "ðŸ“Š Dataset Statistics:\n",
      "   Total images: 17125\n",
      "   Labeled images: 2913\n",
      "Configuration:\n",
      "{\n",
      "  \"voc_root\": \"huanghanchina_pascal_voc_2012_path\",\n",
      "  \"num_classes\": 21,\n",
      "  \"use_pairwise\": true,\n",
      "  \"batch_size\": 4,\n",
      "  \"num_workers\": 4,\n",
      "  \"learning_rate\": 0.001,\n",
      "  \"lr_pretrained\": 0.0001,\n",
      "  \"weight_decay\": 0.0005,\n",
      "  \"stage1_epochs\": 2,\n",
      "  \"stage2_epochs\": 1,\n",
      "  \"stage3_epochs\": 1,\n",
      "  \"image_size\": [\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"base_size\": 520,\n",
      "  \"crop_size\": 512,\n",
      "  \"scale_range\": [\n",
      "    0.5,\n",
      "    2.0\n",
      "  ],\n",
      "  \"use_class_weights\": true,\n",
      "  \"max_train_images\": 100,\n",
      "  \"max_val_images\": 50\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 - Configuration\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "# Pascal VOC class names\n",
    "PASCAL_VOC_CLASSES = [\n",
    "    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n",
    "    'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog',\n",
    "    'horse', 'motorbike', 'person', 'pottedplant', 'sheep',\n",
    "    'sofa', 'train', 'tvmonitor'\n",
    "]\n",
    "\n",
    "import kagglehub\n",
    "huanghanchina_pascal_voc_2012_path = kagglehub.dataset_download('huanghanchina/pascal-voc-2012')\n",
    "print('Data source import complete.')\n",
    "\n",
    "import os\n",
    "\n",
    "# After kagglehub download\n",
    "base_path = huanghanchina_pascal_voc_2012_path\n",
    "print(f\"Dataset downloaded to: {base_path}\")\n",
    "\n",
    "# Configure paths\n",
    "image_dir = os.path.join(base_path,  'VOC2012', 'JPEGImages')\n",
    "label_dir = os.path.join(base_path,  'VOC2012', 'SegmentationClass')\n",
    "train_list = os.path.join(base_path,  'VOC2012', 'ImageSets', 'Segmentation', 'train.txt')\n",
    "val_list = os.path.join(base_path,  'VOC2012', 'ImageSets', 'Segmentation', 'val.txt')\n",
    "\n",
    "# Verify paths exist\n",
    "assert os.path.exists(image_dir), f\"Image directory not found: {image_dir}\"\n",
    "assert os.path.exists(label_dir), f\"Label directory not found: {label_dir}\"\n",
    "print(f\"âœ… Image directory: {image_dir}\")\n",
    "print(f\"âœ… Label directory: {label_dir}\")\n",
    "\n",
    "# Check dataset statistics\n",
    "image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
    "label_files = [f for f in os.listdir(label_dir) if f.endswith('.png')]\n",
    "print(f\"\\nðŸ“Š Dataset Statistics:\")\n",
    "print(f\"   Total images: {len(image_files)}\")\n",
    "print(f\"   Labeled images: {len(label_files)}\")\n",
    "\n",
    "# Training configuration\n",
    "config = {\n",
    "    # Dataset paths (UPDATE THESE!)\n",
    "    'voc_root': 'huanghanchina_pascal_voc_2012_path',  \n",
    "    \n",
    "    # Model parameters\n",
    "    'num_classes': 21,\n",
    "    'use_pairwise': True,\n",
    "    \n",
    "    # Training parameters\n",
    "    'batch_size': 4,  # Reduce if OOM\n",
    "    'num_workers': 4,\n",
    "    'learning_rate': 1e-3,\n",
    "    'lr_pretrained': 1e-4,  # Lower LR for pretrained layers\n",
    "    'weight_decay': 5e-4,\n",
    "    \n",
    "    # Piecewise training epochs\n",
    "    'stage1_epochs': 2,   # Unary training (increase to 20 for full training)\n",
    "    'stage2_epochs': 1,   # Pairwise training (increase to 10 for full training)\n",
    "    'stage3_epochs': 1,   # Joint fine-tuning (increase to 5 for full training)\n",
    "    \n",
    "    # Data augmentation\n",
    "    'image_size': (512, 512),\n",
    "    'base_size': 520,\n",
    "    'crop_size': 512,\n",
    "    'scale_range': (0.5, 2.0),\n",
    "    \n",
    "    # Class weighting\n",
    "    'use_class_weights': True,\n",
    "    \n",
    "    # Debugging (set to None for full dataset)\n",
    "    'max_train_images': 100,  # Use small subset for testing (set to None for full)\n",
    "    'max_val_images': 50,     # Use small subset for testing (set to None for full)\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dfa7779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset classes defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 - Dataset Classes\n",
    "\n",
    "class SegmentationAugmentation:\n",
    "    \"\"\"Data augmentation for semantic segmentation.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_size=520, crop_size=512, scale_range=(0.5, 2.0)):\n",
    "        self.base_size = base_size\n",
    "        self.crop_size = crop_size\n",
    "        self.scale_range = scale_range\n",
    "    \n",
    "    def __call__(self, image, label):\n",
    "        # Random scaling\n",
    "        scale = random.uniform(*self.scale_range)\n",
    "        w, h = image.size\n",
    "        new_w, new_h = int(w * scale), int(h * scale)\n",
    "        \n",
    "        image = image.resize((new_w, new_h), Image.BILINEAR)\n",
    "        label = label.resize((new_w, new_h), Image.NEAREST)\n",
    "        \n",
    "        # Random crop\n",
    "        w, h = image.size\n",
    "        if w > self.crop_size and h > self.crop_size:\n",
    "            x = random.randint(0, w - self.crop_size)\n",
    "            y = random.randint(0, h - self.crop_size)\n",
    "            image = image.crop((x, y, x + self.crop_size, y + self.crop_size))\n",
    "            label = label.crop((x, y, x + self.crop_size, y + self.crop_size))\n",
    "        else:\n",
    "            # Pad if too small\n",
    "            image = image.resize((self.crop_size, self.crop_size), Image.BILINEAR)\n",
    "            label = label.resize((self.crop_size, self.crop_size), Image.NEAREST)\n",
    "        \n",
    "        # Random horizontal flip\n",
    "        if random.random() > 0.5:\n",
    "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            label = label.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "class EnhancedSegmentationDataset(Dataset):\n",
    "    \"\"\"Enhanced dataset with split file support and optional augmentation.\"\"\"\n",
    "    \n",
    "    def __init__(self, image_dir, label_dir, split_list_file=None, \n",
    "                 augmentation=None, image_size=(512, 512), max_images=None):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.label_dir = Path(label_dir)\n",
    "        self.augmentation = augmentation\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Load image IDs from split file\n",
    "        if split_list_file and Path(split_list_file).exists():\n",
    "            with open(split_list_file, 'r') as f:\n",
    "                self.image_ids = [line.strip() for line in f.readlines()]\n",
    "        else:\n",
    "            # Fallback: use all images\n",
    "            self.image_ids = [f.stem for f in self.image_dir.glob('*.jpg')]\n",
    "        \n",
    "        # Limit dataset size if specified (for debugging)\n",
    "        if max_images is not None:\n",
    "            self.image_ids = self.image_ids[:max_images]\n",
    "        \n",
    "        # Normalization\n",
    "        self.normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        \n",
    "        # Load image and label\n",
    "        img_path = self.image_dir / f'{img_id}.jpg'\n",
    "        label_path = self.label_dir / f'{img_id}.png'\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = Image.open(label_path)\n",
    "        \n",
    "        # Apply augmentation\n",
    "        if self.augmentation:\n",
    "            image, label = self.augmentation(image, label)\n",
    "        else:\n",
    "            # Just resize\n",
    "            image = image.resize(self.image_size, Image.BILINEAR)\n",
    "            label = label.resize(self.image_size, Image.NEAREST)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        image = transforms.ToTensor()(image)\n",
    "        label = torch.from_numpy(np.array(label, dtype=np.int64))\n",
    "        \n",
    "        # Normalize image\n",
    "        image = self.normalize(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "def compute_class_weights(dataset, num_classes, ignore_index=255):\n",
    "    \"\"\"Compute class weights for handling class imbalance.\"\"\"\n",
    "    print(\"Computing class weights (this may take a few minutes)...\")\n",
    "    \n",
    "    class_counts = torch.zeros(num_classes)\n",
    "    \n",
    "    for idx in tqdm(range(len(dataset)), desc=\"Scanning dataset\"):\n",
    "        _, label = dataset[idx]\n",
    "        for c in range(num_classes):\n",
    "            class_counts[c] += (label == c).sum().item()\n",
    "    \n",
    "    # Compute weights (inverse frequency)\n",
    "    total_pixels = class_counts.sum()\n",
    "    class_weights = total_pixels / (num_classes * class_counts)\n",
    "    \n",
    "    # Normalize weights\n",
    "    class_weights = class_weights / class_weights.sum() * num_classes\n",
    "    \n",
    "    # Clip extreme weights\n",
    "    class_weights = torch.clamp(class_weights, 0.1, 10.0)\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "\n",
    "print(\"âœ… Dataset classes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "568decae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model architecture defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 - Model Architecture\n",
    "\n",
    "class FeatMapNet(nn.Module):\n",
    "    \"\"\"Feature extraction network based on VGG-16.\"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained VGG-16\n",
    "        vgg = vgg16(pretrained=pretrained)\n",
    "        \n",
    "        # Extract feature layers (conv1 to pool5)\n",
    "        self.block1 = nn.Sequential(*list(vgg.features[:5]))   # conv1_1, conv1_2, pool1\n",
    "        self.block2 = nn.Sequential(*list(vgg.features[5:10]))  # conv2_1, conv2_2, pool2\n",
    "        self.block3 = nn.Sequential(*list(vgg.features[10:17])) # conv3_1, conv3_2, conv3_3, pool3\n",
    "        self.block4 = nn.Sequential(*list(vgg.features[17:24])) # conv4_1, conv4_2, conv4_3, pool4\n",
    "        self.block5 = nn.Sequential(*list(vgg.features[24:31])) # conv5_1, conv5_2, conv5_3, pool5\n",
    "        \n",
    "        # Additional dilated convolution block\n",
    "        self.block6 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UnaryNet(nn.Module):\n",
    "    \"\"\"Unary potential network with multi-scale and spatial pyramid pooling.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=512, num_classes=21, scales=[1.0, 0.5], pool_sizes=[1, 2]):\n",
    "        super().__init__()\n",
    "        self.scales = scales\n",
    "        self.pool_sizes = pool_sizes\n",
    "        \n",
    "        # Convolutional layers for unary potentials\n",
    "        self.conv1 = nn.Conv2d(in_channels, 512, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(512, num_classes, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, feat):\n",
    "        # Multi-scale processing\n",
    "        outputs = []\n",
    "        for scale in self.scales:\n",
    "            if scale != 1.0:\n",
    "                h, w = feat.shape[2:]\n",
    "                scaled_feat = F.interpolate(feat, size=(int(h*scale), int(w*scale)), \n",
    "                                           mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                scaled_feat = feat\n",
    "            \n",
    "            # Spatial pyramid pooling\n",
    "            pooled_feats = []\n",
    "            for pool_size in self.pool_sizes:\n",
    "                if pool_size > 1:\n",
    "                    pooled = F.adaptive_avg_pool2d(scaled_feat, pool_size)\n",
    "                    pooled = F.interpolate(pooled, size=scaled_feat.shape[2:], \n",
    "                                          mode='bilinear', align_corners=False)\n",
    "                    pooled_feats.append(pooled)\n",
    "                else:\n",
    "                    pooled_feats.append(scaled_feat)\n",
    "            \n",
    "            # Combine pooled features\n",
    "            combined = torch.cat(pooled_feats, dim=1) if len(pooled_feats) > 1 else pooled_feats[0]\n",
    "            \n",
    "            # Reduce channels back to original\n",
    "            if combined.shape[1] != feat.shape[1]:\n",
    "                combined = F.conv2d(combined, \n",
    "                                   torch.ones(feat.shape[1], combined.shape[1], 1, 1).to(feat.device) / combined.shape[1])\n",
    "            \n",
    "            # Apply convolutions\n",
    "            x = self.relu(self.conv1(combined))\n",
    "            x = self.relu(self.conv2(x))\n",
    "            x = self.conv3(x)\n",
    "            \n",
    "            # Resize back to original feature size\n",
    "            if scale != 1.0:\n",
    "                x = F.interpolate(x, size=feat.shape[2:], mode='bilinear', align_corners=False)\n",
    "            \n",
    "            outputs.append(x)\n",
    "        \n",
    "        # Average multi-scale outputs\n",
    "        return torch.mean(torch.stack(outputs), dim=0)\n",
    "\n",
    "\n",
    "class PairwiseNet(nn.Module):\n",
    "    \"\"\"Pairwise potential network.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=512, num_classes=21):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, 256, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(256, num_classes, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, feat):\n",
    "        x = self.relu(self.conv1(feat))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AlternativeCRFModel(nn.Module):\n",
    "    \"\"\"Complete alternative CRF model.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=21, use_pairwise=True, scales=[1.0, 0.5], pool_sizes=[1, 2]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.use_pairwise = use_pairwise\n",
    "        \n",
    "        # Networks\n",
    "        self.featmap_net = FeatMapNet(pretrained=True)\n",
    "        self.unary_net = UnaryNet(in_channels=512, num_classes=num_classes, \n",
    "                                  scales=scales, pool_sizes=pool_sizes)\n",
    "        \n",
    "        if use_pairwise:\n",
    "            self.pairwise_net = PairwiseNet(in_channels=512, num_classes=num_classes)\n",
    "    \n",
    "    def forward(self, x, return_features=False):\n",
    "        # Extract features\n",
    "        feat = self.featmap_net(x)\n",
    "        \n",
    "        # Compute unary potentials\n",
    "        unary = self.unary_net(feat)\n",
    "        \n",
    "        # Upsample to input size\n",
    "        unary = F.interpolate(unary, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        if return_features:\n",
    "            return unary, feat\n",
    "        return unary\n",
    "    \n",
    "    def get_pairwise_scores(self, feat):\n",
    "        \"\"\"Get pairwise potential scores.\"\"\"\n",
    "        if not self.use_pairwise:\n",
    "            return None\n",
    "        return self.pairwise_net(feat)\n",
    "\n",
    "\n",
    "# Import SegmentationMetrics from existing code\n",
    "from src.piecewise_training.metrics import SegmentationMetrics\n",
    "\n",
    "print(\"âœ… Model architecture defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e921a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n",
      "\n",
      "ðŸ“Š Dataset Statistics:\n",
      "   Training samples: 100\n",
      "   Validation samples: 50\n",
      "âœ… Data loaders created!\n",
      "   Train batches: 25\n",
      "   Val batches: 13\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 - Create Datasets & Data Loaders (FIXED)\n",
    "\n",
    "print(\"Creating datasets...\")\n",
    "\n",
    "# Training augmentation\n",
    "train_aug = SegmentationAugmentation(\n",
    "    base_size=config['base_size'],\n",
    "    crop_size=config['crop_size'],\n",
    "    scale_range=config['scale_range']\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = EnhancedSegmentationDataset(\n",
    "    image_dir=image_dir,\n",
    "    label_dir=label_dir,\n",
    "    split_list_file=train_list,\n",
    "    augmentation=train_aug,\n",
    "    image_size=config['image_size'],\n",
    "    max_images=config['max_train_images']\n",
    ")\n",
    "\n",
    "val_dataset = EnhancedSegmentationDataset(\n",
    "    image_dir=image_dir,\n",
    "    label_dir=label_dir,\n",
    "    split_list_file=val_list,\n",
    "    augmentation=None,  # No augmentation for validation\n",
    "    image_size=config['image_size'],\n",
    "    max_images=config['max_val_images']\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset Statistics:\")\n",
    "print(f\"   Training samples: {len(train_dataset)}\")\n",
    "print(f\"   Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Create data loaders (FIX: Set num_workers=0 for Windows)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # âœ… Changed from 4 to 0 (fixes Windows multiprocessing issue)\n",
    "    pin_memory=False,  # âœ… Changed from True to False\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # âœ… Changed from 4 to 0\n",
    "    pin_memory=False  # âœ… Changed from True to False\n",
    ")\n",
    "\n",
    "print(f\"âœ… Data loaders created!\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f249571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Computing class weights for handling class imbalance...\n",
      "======================================================================\n",
      "Computing class weights (this may take a few minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 40.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Weights:\n",
      "   background     : 0.1000\n",
      "   aeroplane      : 0.7460\n",
      "   bicycle        : 2.7984\n",
      "   bird           : 0.8267\n",
      "   boat           : 1.5631\n",
      "   bottle         : 1.7627\n",
      "   bus            : 0.3974\n",
      "   car            : 0.6645\n",
      "   cat            : 0.3138\n",
      "   chair          : 1.2500\n",
      "   cow            : 0.9179\n",
      "   diningtable    : 0.3978\n",
      "   dog            : 1.2287\n",
      "   horse          : 0.9798\n",
      "   motorbike      : 0.4705\n",
      "   person         : 0.1438\n",
      "   pottedplant    : 2.5484\n",
      "   sheep          : 0.8606\n",
      "   sofa           : 0.8830\n",
      "   train          : 1.6133\n",
      "   tvmonitor      : 0.6207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 - Compute Class Weights \n",
    "\n",
    "class_weights = None\n",
    "if config['use_class_weights']:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Computing class weights for handling class imbalance...\")\n",
    "    print(\"=\"*70)\n",
    "    class_weights = compute_class_weights(train_dataset, config['num_classes'])\n",
    "    class_weights = class_weights.to(device)\n",
    "    \n",
    "    # Display class weights\n",
    "    print(\"\\nClass Weights:\")\n",
    "    for idx, (name, weight) in enumerate(zip(PASCAL_VOC_CLASSES, class_weights)):\n",
    "        print(f\"   {name:15s}: {weight:.4f}\")\n",
    "else:\n",
    "    print(\"âš ï¸  Not using class weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc843a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CREATING MODEL\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š Model Statistics:\n",
      "   Total parameters: 28,299,882\n",
      "   Trainable parameters: 28,299,882\n",
      "   Model size: 107.96 MB (FP32)\n",
      "\n",
      "âœ… Model forward pass successful!\n",
      "   Input shape: torch.Size([1, 3, 512, 512])\n",
      "   Output shape: torch.Size([1, 21, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 - Create Model\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model = AlternativeCRFModel(\n",
    "    num_classes=config['num_classes'],\n",
    "    use_pairwise=True,\n",
    "    scales=[1.0, 0.5],  # Multi-scale processing\n",
    "    pool_sizes=[1, 2]   # Spatial pyramid pooling\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nðŸ“Š Model Statistics:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Model size: {total_params * 4 / 1024 / 1024:.2f} MB (FP32)\")\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    test_input = torch.randn(1, 3, *config['image_size']).to(device)\n",
    "    test_output = model(test_input)\n",
    "    print(f\"\\nâœ… Model forward pass successful!\")\n",
    "    print(f\"   Input shape: {test_input.shape}\")\n",
    "    print(f\"   Output shape: {test_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9430557d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Trainer class defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 10 - Trainer Implementation (FIXED)\n",
    "\n",
    "class AlternativeTrainer:\n",
    "    \"\"\"Trainer for the alternative CRF model with piecewise training.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, device, num_classes, learning_rate=1e-3, \n",
    "                 lr_pretrained=1e-4, weight_decay=5e-4, class_weights=None):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_pretrained = lr_pretrained\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # Loss functions\n",
    "        self.unary_loss_fn = nn.CrossEntropyLoss(\n",
    "            ignore_index=255,\n",
    "            weight=class_weights\n",
    "        )\n",
    "        self.pairwise_loss_fn = nn.CrossEntropyLoss(\n",
    "            ignore_index=255,\n",
    "            weight=class_weights\n",
    "        )\n",
    "        \n",
    "        # Metrics\n",
    "        self.metrics = SegmentationMetrics(num_classes)\n",
    "    \n",
    "    def _get_optimizer_stage1(self):\n",
    "        \"\"\"Get optimizer for stage 1 (unary training).\"\"\"\n",
    "        pretrained_params = []\n",
    "        new_params = []\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'featmap_net.block' in name and 'block6' not in name:\n",
    "                pretrained_params.append(param)\n",
    "            else:\n",
    "                new_params.append(param)\n",
    "        \n",
    "        return torch.optim.SGD([\n",
    "            {'params': pretrained_params, 'lr': self.lr_pretrained},\n",
    "            {'params': new_params, 'lr': self.learning_rate}\n",
    "        ], momentum=0.9, weight_decay=self.weight_decay)\n",
    "    \n",
    "    def _get_optimizer_stage2(self):\n",
    "        \"\"\"Get optimizer for stage 2 (pairwise training).\"\"\"\n",
    "        return torch.optim.SGD(\n",
    "            self.model.pairwise_net.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            momentum=0.9,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "    \n",
    "    def _get_optimizer_stage3(self):\n",
    "        \"\"\"Get optimizer for stage 3 (joint fine-tuning).\"\"\"\n",
    "        trainable_params = list(self.model.unary_net.parameters()) + \\\n",
    "                          list(self.model.pairwise_net.parameters())\n",
    "        \n",
    "        return torch.optim.SGD(\n",
    "            trainable_params,\n",
    "            lr=self.learning_rate * 0.1,\n",
    "            momentum=0.9,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "    \n",
    "    def train_stage1_unary(self, train_loader, num_epochs, val_loader=None):\n",
    "        \"\"\"Stage 1: Train unary network.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STAGE 1: Training Unary Potentials\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        optimizer = self._get_optimizer_stage1()\n",
    "        history = {'train_loss': [], 'val_miou': [], 'val_pixel_acc': []}\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            for imgs, labels in pbar:\n",
    "                imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                unary, _ = self.model(imgs, return_features=True)\n",
    "                loss = self.unary_loss_fn(unary, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            history['train_loss'].append(avg_loss)\n",
    "            \n",
    "            if val_loader is not None:\n",
    "                val_metrics = self.validate(val_loader, use_pairwise=False)\n",
    "                history['val_miou'].append(val_metrics['miou'])  # âœ… Changed from 'mIoU'\n",
    "                history['val_pixel_acc'].append(val_metrics['pixel_acc'])\n",
    "                \n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "                      f\"Loss: {avg_loss:.4f}, \"\n",
    "                      f\"Val mIoU: {val_metrics['miou']:.4f}, \"\n",
    "                      f\"Val Acc: {val_metrics['pixel_acc']:.4f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def train_stage2_pairwise(self, train_loader, num_epochs, val_loader=None):\n",
    "        \"\"\"Stage 2: Train pairwise network.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STAGE 2: Training Pairwise Potentials\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Freeze unary network\n",
    "        for param in self.model.featmap_net.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.model.unary_net.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        optimizer = self._get_optimizer_stage2()\n",
    "        history = {'train_loss': [], 'val_miou': [], 'val_pixel_acc': []}\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            for imgs, labels in pbar:\n",
    "                imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                unary, feat = self.model(imgs, return_features=True)\n",
    "                pairwise = self.model.get_pairwise_scores(feat)\n",
    "                \n",
    "                # Upsample pairwise to match label size\n",
    "                if pairwise.shape[2:] != labels.shape[1:]:\n",
    "                    pairwise = F.interpolate(pairwise, size=labels.shape[1:], \n",
    "                                            mode='bilinear', align_corners=False)\n",
    "                \n",
    "                loss = self.pairwise_loss_fn(pairwise, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            history['train_loss'].append(avg_loss)\n",
    "            \n",
    "            if val_loader is not None:\n",
    "                val_metrics = self.validate(val_loader, use_pairwise=True)\n",
    "                history['val_miou'].append(val_metrics['miou'])  # âœ… Changed from 'mIoU'\n",
    "                history['val_pixel_acc'].append(val_metrics['pixel_acc'])\n",
    "                \n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "                      f\"Loss: {avg_loss:.4f}, \"\n",
    "                      f\"Val mIoU: {val_metrics['miou']:.4f}, \"\n",
    "                      f\"Val Acc: {val_metrics['pixel_acc']:.4f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Unfreeze for next stage\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def train_stage3_joint(self, train_loader, num_epochs, val_loader=None):\n",
    "        \"\"\"Stage 3: Joint fine-tuning.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STAGE 3: Joint Fine-tuning\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Freeze feature extractor\n",
    "        for param in self.model.featmap_net.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        optimizer = self._get_optimizer_stage3()\n",
    "        history = {'train_loss': [], 'val_miou': [], 'val_pixel_acc': []}\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            for imgs, labels in pbar:\n",
    "                imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                unary, feat = self.model(imgs, return_features=True)\n",
    "                pairwise = self.model.get_pairwise_scores(feat)\n",
    "                \n",
    "                # Upsample pairwise\n",
    "                if pairwise.shape[2:] != labels.shape[1:]:\n",
    "                    pairwise = F.interpolate(pairwise, size=labels.shape[1:], \n",
    "                                            mode='bilinear', align_corners=False)\n",
    "                \n",
    "                # Combined loss\n",
    "                loss_u = self.unary_loss_fn(unary, labels)\n",
    "                loss_p = self.pairwise_loss_fn(pairwise, labels)\n",
    "                loss = loss_u + 0.5 * loss_p\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            history['train_loss'].append(avg_loss)\n",
    "            \n",
    "            if val_loader is not None:\n",
    "                val_metrics = self.validate(val_loader, use_pairwise=True)\n",
    "                history['val_miou'].append(val_metrics['miou'])  # âœ… Changed from 'mIoU'\n",
    "                history['val_pixel_acc'].append(val_metrics['pixel_acc'])\n",
    "                \n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "                      f\"Loss: {avg_loss:.4f}, \"\n",
    "                      f\"Val mIoU: {val_metrics['miou']:.4f}, \"\n",
    "                      f\"Val Acc: {val_metrics['pixel_acc']:.4f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def validate(self, val_loader, use_pairwise=False):\n",
    "        \"\"\"Validate the model.\"\"\"\n",
    "        self.model.eval()\n",
    "        self.metrics.reset()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in val_loader:\n",
    "                imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                if use_pairwise:\n",
    "                    unary, feat = self.model(imgs, return_features=True)\n",
    "                    pairwise = self.model.get_pairwise_scores(feat)\n",
    "                    if pairwise.shape[2:] != labels.shape[1:]:\n",
    "                        pairwise = F.interpolate(pairwise, size=labels.shape[1:], \n",
    "                                                mode='bilinear', align_corners=False)\n",
    "                    preds = (unary + pairwise).argmax(dim=1)\n",
    "                else:\n",
    "                    unary = self.model(imgs)\n",
    "                    preds = unary.argmax(dim=1)\n",
    "                \n",
    "                self.metrics.update(preds, labels)\n",
    "        \n",
    "        # âœ… FIX: Use correct method names from SegmentationMetrics\n",
    "        miou = self.metrics.compute_miou()\n",
    "        pixel_acc = self.metrics.compute_pixel_accuracy()\n",
    "        iou_per_class = self.metrics.compute_iou()\n",
    "        \n",
    "        return {\n",
    "            'miou': miou,\n",
    "            'pixel_acc': pixel_acc,\n",
    "            'iou_per_class': iou_per_class\n",
    "        }\n",
    "    \n",
    "    def train_piecewise(self, train_loader, stage1_epochs=2, stage2_epochs=2, \n",
    "                       stage3_epochs=2, val_loader=None):\n",
    "        \"\"\"Run complete piecewise training pipeline.\"\"\"\n",
    "        history = {'stage1': {}, 'stage2': {}, 'stage3': {}}\n",
    "        \n",
    "        history['stage1'] = self.train_stage1_unary(train_loader, stage1_epochs, val_loader)\n",
    "        history['stage2'] = self.train_stage2_pairwise(train_loader, stage2_epochs, val_loader)\n",
    "        history['stage3'] = self.train_stage3_joint(train_loader, stage3_epochs, val_loader)\n",
    "        \n",
    "        return history\n",
    "\n",
    "print(\"âœ… Trainer class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96fd422b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CREATING TRAINER\n",
      "======================================================================\n",
      "âœ… Trainer initialized!\n",
      "\n",
      "Training Configuration:\n",
      "   Stage 1 (Unary): 2 epochs\n",
      "   Stage 2 (Pairwise): 1 epochs\n",
      "   Stage 3 (Joint): 1 epochs\n",
      "   Total: 4 epochs\n"
     ]
    }
   ],
   "source": [
    "# Cell 11 - Create Trainer\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING TRAINER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "trainer = AlternativeTrainer(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    num_classes=config['num_classes'],\n",
    "    learning_rate=config['learning_rate'],\n",
    "    lr_pretrained=config['lr_pretrained'],\n",
    "    weight_decay=config['weight_decay'],\n",
    "    class_weights=class_weights\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer initialized!\")\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"   Stage 1 (Unary): {config['stage1_epochs']} epochs\")\n",
    "print(f\"   Stage 2 (Pairwise): {config['stage2_epochs']} epochs\")\n",
    "print(f\"   Stage 3 (Joint): {config['stage3_epochs']} epochs\")\n",
    "print(f\"   Total: {config['stage1_epochs'] + config['stage2_epochs'] + config['stage3_epochs']} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abe39b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STARTING PIECEWISE TRAINING\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STAGE 1: Training Unary Potentials\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:09<00:00,  2.55it/s, loss=3.0063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Loss: 3.0193, Val mIoU: 0.0378, Val Acc: 0.7188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:09<00:00,  2.76it/s, loss=3.0159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Loss: 2.9972, Val mIoU: 0.0378, Val Acc: 0.7188\n",
      "\n",
      "======================================================================\n",
      "STAGE 2: Training Pairwise Potentials\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.52it/s, loss=3.0477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 - Loss: 3.0335, Val mIoU: 0.0378, Val Acc: 0.7188\n",
      "\n",
      "======================================================================\n",
      "STAGE 3: Joint Fine-tuning\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:06<00:00,  3.78it/s, loss=4.3905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 - Loss: 4.4921, Val mIoU: 0.0378, Val Acc: 0.7188\n",
      "\n",
      "======================================================================\n",
      "âœ… TRAINING COMPLETE!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 12 - RUN PIECEWISE TRAINING ðŸš€\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING PIECEWISE TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Run piecewise training\n",
    "history = trainer.train_piecewise(\n",
    "    train_loader=train_loader,\n",
    "    stage1_epochs=config['stage1_epochs'],\n",
    "    stage2_epochs=config['stage2_epochs'],\n",
    "    stage3_epochs=config['stage3_epochs'],\n",
    "    val_loader=val_loader\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7611191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13 - Save Model\n",
    "\n",
    "save_dir = Path('alternative_model_results')\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model_path = save_dir / 'alternative_model_final.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Save training history\n",
    "history_path = save_dir / 'training_history.json'\n",
    "with open(history_path, 'w') as f:\n",
    "    # Convert numpy arrays to lists for JSON serialization\n",
    "    history_json = {}\n",
    "    for stage, metrics in history.items():\n",
    "        history_json[stage] = {k: [float(v) for v in vals] \n",
    "                              for k, vals in metrics.items()}\n",
    "    json.dump(history_json, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Model saved to: {model_path}\")\n",
    "print(f\"âœ… History saved to: {history_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6754ac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14 - Plot Training Curves\n",
    "\n",
    "def plot_training_curves(history):\n",
    "    \"\"\"Plot training curves for all stages.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    stages = ['stage1', 'stage2', 'stage3']\n",
    "    stage_names = ['Stage 1: Unary', 'Stage 2: Pairwise', 'Stage 3: Joint']\n",
    "    \n",
    "    for idx, (stage, name) in enumerate(zip(stages, stage_names)):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Plot training loss\n",
    "        ax.plot(history[stage]['train_loss'], label='Train Loss', linewidth=2)\n",
    "        \n",
    "        # Plot validation metrics if available\n",
    "        if 'val_miou' in history[stage] and history[stage]['val_miou']:\n",
    "            ax2 = ax.twinx()\n",
    "            ax2.plot(history[stage]['val_miou'], 'g-', label='Val mIoU', linewidth=2)\n",
    "            ax2.set_ylabel('mIoU', fontsize=12)\n",
    "            ax2.legend(loc='upper right')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        ax.set_xlabel('Epoch', fontsize=12)\n",
    "        ax.set_ylabel('Loss', fontsize=12)\n",
    "        ax.set_title(name, fontsize=14, fontweight='bold')\n",
    "        ax.legend(loc='upper left')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_training_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07df446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15 - Final Evaluation\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluate on validation set\n",
    "final_metrics = trainer.validate(val_loader, use_pairwise=True)\n",
    "\n",
    "print(f\"\\nðŸ“Š Final Metrics:\")\n",
    "print(f\"   Mean IoU: {final_metrics['mIoU']:.4f}\")\n",
    "print(f\"   Pixel Accuracy: {final_metrics['pixel_acc']:.4f}\")\n",
    "\n",
    "# Per-class IoU\n",
    "print(f\"\\nðŸ“‹ Per-Class IoU:\")\n",
    "for idx, (name, iou) in enumerate(zip(PASCAL_VOC_CLASSES, final_metrics['iou_per_class'])):\n",
    "    if iou > 0:\n",
    "        print(f\"   {name:15s}: {iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e28855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16 - Visualize Predictions\n",
    "\n",
    "def visualize_predictions(model, dataset, device, num_samples=4):\n",
    "    \"\"\"Visualize model predictions.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    indices = random.sample(range(len(dataset)), num_samples)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, sample_idx in enumerate(indices):\n",
    "            image, label = dataset[sample_idx]\n",
    "            image_input = image.unsqueeze(0).to(device)\n",
    "            \n",
    "            # Get predictions\n",
    "            unary, feat = model(image_input, return_features=True)\n",
    "            pairwise = model.get_pairwise_scores(feat)\n",
    "            \n",
    "            # Upsample pairwise\n",
    "            if pairwise.shape[2:] != unary.shape[2:]:\n",
    "                pairwise = F.interpolate(pairwise, size=unary.shape[2:], \n",
    "                                        mode='bilinear', align_corners=False)\n",
    "            \n",
    "            pred_unary = unary.argmax(1).squeeze(0).cpu()\n",
    "            pred_combined = (unary + pairwise).argmax(1).squeeze(0).cpu()\n",
    "            \n",
    "            # Denormalize image\n",
    "            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "            image_denorm = image * std + mean\n",
    "            image_denorm = torch.clamp(image_denorm, 0, 1)\n",
    "            \n",
    "            # Plot\n",
    "            axes[idx, 0].imshow(image_denorm.permute(1, 2, 0))\n",
    "            axes[idx, 0].set_title('Input Image', fontweight='bold')\n",
    "            axes[idx, 0].axis('off')\n",
    "            \n",
    "            axes[idx, 1].imshow(label, cmap='tab20', vmin=0, vmax=20)\n",
    "            axes[idx, 1].set_title('Ground Truth', fontweight='bold')\n",
    "            axes[idx, 1].axis('off')\n",
    "            \n",
    "            axes[idx, 2].imshow(pred_unary, cmap='tab20', vmin=0, vmax=20)\n",
    "            axes[idx, 2].set_title('Unary Only', fontweight='bold')\n",
    "            axes[idx, 2].axis('off')\n",
    "            \n",
    "            axes[idx, 3].imshow(pred_combined, cmap='tab20', vmin=0, vmax=20)\n",
    "            axes[idx, 3].set_title('Unary + Pairwise', fontweight='bold')\n",
    "            axes[idx, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / 'sample_predictions.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "visualize_predictions(model, val_dataset, device, num_samples=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00401a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17 - Inference Function\n",
    "\n",
    "def run_inference(model, image_path, device):\n",
    "    \"\"\"Run inference on a single image.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    original_size = image.size\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(config['image_size']),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        unary, feat = model(image_tensor, return_features=True)\n",
    "        pairwise = model.get_pairwise_scores(feat)\n",
    "        \n",
    "        if pairwise.shape[2:] != unary.shape[2:]:\n",
    "            pairwise = F.interpolate(pairwise, size=unary.shape[2:], \n",
    "                                    mode='bilinear', align_corners=False)\n",
    "        \n",
    "        pred = (unary + pairwise).argmax(1).squeeze(0).cpu()\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title('Input Image', fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(pred, cmap='tab20', vmin=0, vmax=20)\n",
    "    axes[1].set_title('Prediction', fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pred\n",
    "\n",
    "# Example usage (update path to your test image)\n",
    "# test_image_path = '/path/to/test/image.jpg'\n",
    "# if os.path.exists(test_image_path):\n",
    "#     prediction = run_inference(model, test_image_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a9620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18 - Summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "âœ… Model trained successfully with piecewise strategy\n",
    "âœ… Final mIoU: {final_metrics['mIoU']:.4f}\n",
    "âœ… Pixel Accuracy: {final_metrics['pixel_acc']:.4f}\n",
    "\n",
    "ðŸ“ Generated Files:\n",
    "   - Model: {model_path}\n",
    "   - History: {history_path}\n",
    "   - Training curves: {save_dir / 'training_curves.png'}\n",
    "   - Sample predictions: {save_dir / 'sample_predictions.png'}\n",
    "\n",
    "ðŸŽ¯ Next Steps:\n",
    "   1. Review training curves and metrics\n",
    "   2. Analyze per-class performance\n",
    "   3. Run inference on your own images\n",
    "   4. Fine-tune hyperparameters if needed\n",
    "   5. Experiment with different scales and pool sizes\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
